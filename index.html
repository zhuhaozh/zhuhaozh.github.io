<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hao Zhu's personal page</title>
  
  <meta name="author" content="Hao Zhu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hao Zhu ("朱昊" in Chinese)</name>
              </p>
              <p>Hao Zhu is currently researcher at <a href="https://www.sensetime.com/en">SenseTime Research</a>, where he works on generative models and neural rendering.
              </p>
              <p>
                He received his master degree from <a href="https://www.ahu.edu.cn/">Anhui University</a> and is also a joint master student at <a href="http://cripac.ia.ac.cn/en/EN/volumn/home.shtml">CRIPAC</a>, <a href="http://english.ia.cas.cn/">CASIA</a>, supervised by <a href="https://rhe-web.github.io/">Prof. Ran He</a> and <a href="https://scholar.google.com.hk/citations?user=vcMHRWEAAAAJ&hl">Prof. Aihua Zheng</a>.
              </p>
              <p>Please feel free to contact me through email if you have any questions. </p>
              <p style="text-align:center">
                <a href="mailto:haozhu96@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=sntA9FgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zhuhaozh/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zh.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zh.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>[Jul 2022]  1 paper was accepted by ECCV 2022. <sup><font color="red"> new</font></sup></p>
            <p>[Jul 2021] Join SenseTime as a fulltime researcher working on generative models and neural rendering. If you are interested in a research internship position, please contact us.</p>
            <p>[Oct 2020] One paper on face swapping was accepted to NeurIPS 2020. </p>
            <p>[Sep 2018] Start my graduate study and research at Anhui University and CRIPAC CASIA. </p>
        </td>
        </tr>
      </tbody></table>

       <!-- News -->

        
      <!-- publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading> <a href="publications.html">[full list]</a>
              <p>
                I'm interested in machine learning, computer vision, and neural rendering. Much of my research is about high-quality generative models. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/celebvhq.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#">
                <papertitle>CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</papertitle>
              </a>
              <br>
              <strong>Hao Zhu</strong>*, 
              <a>Wayne Wu</a><sup>✉</sup>*,
              <a>Wentao Zhu</a>,
              <a>Liming Jiang</a>,
              <a>Siwei Tang</a>,
              <a>Li Zhang</a>,
              <a>Ziwei Liu</a>,
              <a>Chen Change Loy</a>
              <br>
        <em>ECCV</em>, 2022
              <br>
              <a href='http://celebv-hq.github.io/'>Project Page</a>
              <p></p>
              <p> A large-scale, high-quality, and diverse video dataset with rich facial attribute annotations. CelebV-HQ contains 35,666 video clips with the resolution of 512x512 at least. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/survey_ijac21.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#">
                <papertitle>Deep audio-visual learning: A survey</papertitle>
              </a>
              <br>
              <strong>Hao Zhu</strong>, 
              <a href="#">Mandi Luo</a>,
              <a href="#">Rui Wang</a>,
              <a href="#">Aihua Zheng</a>, 
              <a href="#">Ran He</a>
              <br>
        <em>IJAC</em>, 2021
              <br>
              <a href="http://www.ijac.net/en/article/doi/10.1007/s11633-021-1293-0">PDF</a>
              <p></p>
              <p>we provide a comprehensive survey of recent audio-visual learning development. </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/aot_nips20.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="projects/AOT/">
                <papertitle>AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection                </papertitle>
              </a>
              <br>
              <strong>Hao Zhu</strong>* 
              <a href="#">Chaoyou Fu</a>*,
              <a href="https://http://qianyiwu.github.io/">Qianyi Wu</a>,
              <a href="https://wywu.github.io/">Wayne Wu</a>, 
              <a href="#">Chen Qian</a>,
              <a href="https://rhe-web.github.io/">Ran He</a>
              <br>
        <em>NeurIPS</em>, 2020
              <br>
              <a href="projects/AOT">project page</a>
        /
              <a href="https://github.com/zhuhaozh/AOT">Code & Dataset</a>
        /
              <a href="http://arxiv.org/abs/2011.02674">arXiv</a>
              <p></p>
              <p>A new identity swapping algorithm with large differences in appearance for forgery detection.</p>
            </td>
          </tr> 

          <!-- IJCAI'20 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/amie_ijcai20.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#">
                <papertitle>Arbitrary Talking Face Generation via Attentional Audio-visual Coherence Learning</papertitle>
              </a>
              <br>
              <strong>Hao Zhu</strong>,
              <a href="#">Huaibo Huang</a>,
              <a href="#">Yi Li</a>,
              <a href="#">Aihua Zheng</a>,
              <a href="#">Ran He</a>
              <br>
        <em>IJCAI</em>, 2020
              <br>
              <!-- <a href="https://github.com/zhuhaozh/AOT">project page</a> -->
        <!-- / -->
              <a href="http://arxiv.org/abs/1812.06589">arXiv</a>
              <p></p>
              <p>A novel arbitrary talking face generation framework by discovering the audio-visual coherence via the proposed Asymmetric Mutual Information Estimator (AMIE).</p>
            </td>
          </tr> 


        </tbody></table>



          <!-- Awards -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <p>ACF Outstanding Master's Thesis. 2021</p>
            <p>Outstanding Master's Graduate, Anhui Province, China. 2021</p>
            <p>National Scholarship, AHU. 2020. </p>
        </td>
        </tr>
      </tbody></table>

       <!-- Awards -->
      <!-- last info -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template credits: <a href="https://jonbarron.info">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
